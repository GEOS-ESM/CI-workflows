name: Build and Test with NAG

on: workflow_call

defaults:
  run:
    shell: bash

jobs:
  nag-build:

    runs-on: nccs-discover
    timeout-minutes: 30
    env:
      CI_WORKSPACE: /discover/nobackup/gmao_ci/mapl/nag/${{ github.run_id }}

    steps:
      - name: Validate workflow
        run: |
          /home/jardizzo/bin/nams_check.py ${{ github.triggering_actor }} mapl

      - name: Checkout code
        uses: actions/checkout@v5
        with:
          set-safe-directory: false

      # Add a stable global setting that covers the workspace and any mepo clones
      - name: Trust workspace and clones
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          git config --global --add safe.directory '*'

      - name: Copy modules
        run: |
          # Make testing directory
          mkdir -p "${CI_WORKSPACE}"
          # Copy modules
          cp /discover/nobackup/gmao_ci/mapl/nag/modules "${CI_WORKSPACE}/modules"

      - name: Run mepo
        run: |
          source "${CI_WORKSPACE}/modules"
          cd "${GITHUB_WORKSPACE}"
          mepo clone

      - name: Run cmake
        run: |
          source "${CI_WORKSPACE}/modules"
          cmake -S "${GITHUB_WORKSPACE}" -B "${CI_WORKSPACE}/build" -DCMAKE_BUILD_TYPE=Debug --install-prefix "${CI_WORKSPACE}/install" -DUSE_F2PY=OFF

      - name: Submit SLURM build and test
        run: |
          set -euo pipefail
          source "${CI_WORKSPACE}/modules"

          JOBID_FILE="${CI_WORKSPACE}/jobid.txt"
          # Create/empty this early so the trap can use it
          : > "${JOBID_FILE}"

          # Trap cancellation from GitHub Actions and cancel the SLURM job
          cancel_handler() {
            echo "[GHA] Caught cancellation signal; attempting to cancel SLURM job..."
            if [[ -s "${JOBID_FILE}" ]]; then
              JID="$(cat "${JOBID_FILE}")"
              if [[ -n "${JID}" ]]; then
                scancel "${JID}" || true
                echo "[GHA] scancel ${JID} requested."
              else
                echo "[GHA] JOBID file exists but is empty."
              fi
            else
              echo "[GHA] No JOBID recorded yet; nothing to cancel."
            fi
            exit 1
          }
          trap cancel_handler TERM INT

          cat > "${CI_WORKSPACE}/build_test_job.sh" <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          source "${CI_WORKSPACE}/modules"
          cd "${CI_WORKSPACE}/build"

          # Pick something sensible, then clamp to [1,20]
          PAR=${SLURM_CPUS_PER_TASK:-${SLURM_CPUS_ON_NODE:-1}}
          (( PAR = PAR < 1 ? 1 : (PAR > 20 ? 20 : PAR) ))

          echo "[SLURM] Using PAR=${PAR}"

          echo "[SLURM] Building with parallelism: ${PAR}"
          cmake --build "${CI_WORKSPACE}/build" --parallel "${PAR}"
          cmake --install "${CI_WORKSPACE}/build"

          echo "[SLURM] Running ctest"
          cd "${CI_WORKSPACE}/build"
          make -j "${PAR}" build-tests
          make -j "${PAR}" tests
          EOS
          chmod +x "${CI_WORKSPACE}/build_test_job.sh"

          echo "[GHA] Submitting SLURM job..."
          # Submit without --wait so we get the jobid immediately
          JOBID=$(sbatch --parsable \
            --export=ALL,CI_WORKSPACE \
            --job-name="mapl-ci-${GITHUB_RUN_ID}" \
            --constraint=mil \
            --account=s1873 \
            --nodes=1 \
            --time=00:15:00 \
            --output="${CI_WORKSPACE}/slurm-%j.out" \
            --error="${CI_WORKSPACE}/slurm-%j.err" \
            "${CI_WORKSPACE}/build_test_job.sh")

          # Some SLURM configs return "123456.batch" etc; strip any suffix
          JOBID=${JOBID%%.*}
          echo -n "${JOBID}" > "${JOBID_FILE}" || true
          echo "[GHA] Submitted SLURM job ${JOBID}"
          echo "Logs (once finished): ${CI_WORKSPACE}/slurm-${JOBID}.out / ${CI_WORKSPACE}/slurm-${JOBID}.err"

          # ---- Wait for job completion via sacct/squeue ----
          EXIT_CODE=1
          STATE="UNKNOWN"

          # Give sacct a chance to see the job
          sleep 10

          while true; do
            # Try sacct first â€“ more definitive about final state
            STATE=$(sacct -j "${JOBID}" --format=State --noheader 2>/dev/null | head -n 1 | awk '{print $1}')

            if [[ -z "${STATE}" ]]; then
              # Job might not have hit accounting yet; fall back to squeue
              if squeue -h -j "${JOBID}" >/dev/null 2>&1; then
                echo "[GHA] Job ${JOBID} running or pending (no sacct entry yet). Sleeping..."
                sleep 30
                continue
              else
                echo "[GHA] Job ${JOBID} not found in sacct or squeue; assuming failure."
                STATE="UNKNOWN"
                EXIT_CODE=1
                break
              fi
            fi

            case "${STATE}" in
              COMPLETED)
                EXIT_CODE=0
                break
                ;;
              CANCELLED*|FAILED*|TIMEOUT*|PREEMPTED*|NODE_FAIL*|OUT_OF_MEMORY*)
                EXIT_CODE=1
                break
                ;;
              PENDING|CONFIGURING|RUNNING|COMPLETING|SUSPENDED)
                echo "[GHA] Job ${JOBID} state=${STATE}; still waiting..."
                sleep 30
                ;;
              *)
                echo "[GHA] Job ${JOBID} in unexpected state=${STATE}; treating as failure."
                EXIT_CODE=1
                break
                ;;
            esac
          done

          echo "[GHA] SLURM job ${JOBID} finished with state=${STATE}, exit=${EXIT_CODE}"
          exit "${EXIT_CODE}"

      # Show SLURM logs in the Actions UI if anything failed above
      - name: Show SLURM logs on failure
        if: failure()
        run: |
          set -euo pipefail
          JOBID=""
          [[ -f "${CI_WORKSPACE}/jobid.txt" ]] && JOBID=$(cat "${CI_WORKSPACE}/jobid.txt" || true)
          if [[ -n "${JOBID}" ]]; then
            echo "=== slurm-${JOBID}.out (tail 200) ==="
            tail -n 200 "${CI_WORKSPACE}/slurm-${JOBID}.out" || true
            echo "=== slurm-${JOBID}.err (tail 200) ==="
            tail -n 200 "${CI_WORKSPACE}/slurm-${JOBID}.err" || true
          else
            echo "No jobid.txt found; SLURM may have failed before submit."
          fi

      # Preserve the full workspace (including SLURM logs) on failure
      - name: Preserve on failure
        if: failure()
        run: |
          set -euo pipefail
          DEST="${CI_WORKSPACE}_FAILED"
          mv "${CI_WORKSPACE}" "${DEST}" || true
          echo "Preserved failed workspace at ${DEST}"

      - name: Cleanup
        if: always()
        run: |
          set -euo pipefail
          if [ -d "${CI_WORKSPACE}" ]; then
            echo "[CLEANUP] Attempting to remove CI_WORKSPACE"
            for i in {1..5}; do
              rm -rf "${CI_WORKSPACE}" && break || true
              echo "[CLEANUP] Retry $i: directory removal failed; waiting..."
              sleep 5
            done
            if [ -d "${CI_WORKSPACE}" ]; then
              echo "[CLEANUP] Final attempt: using find-delete fallback"
              find "${CI_WORKSPACE}" -type f -delete || true
              find "${CI_WORKSPACE}" -depth -type d -exec rmdir {} \; || true
            fi
          fi

